{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SklearnNotebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/clCJs2okpqrtzk2ZzsIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaspergroenbek98/first-first-year-project/blob/master/Copy_of_SklearnNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLhZtbJFjmmT",
        "colab_type": "code",
        "outputId": "17d72389-9b14-4b43-b8ea-f699bc879881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsbJDxBGjqxk",
        "colab_type": "code",
        "outputId": "027563da-afcd-4e1d-f786-debe39e17550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgeCy9_vYOZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "4f726f92-e6a4-4d43-f3ef-741ff259e50f"
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "first_clean = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "first_clean\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>Resolution_Category</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>tweet_state</th>\n",
              "      <th>tweet_region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>#NewYearsResolution :: Read more books, No scr...</td>\n",
              "      <td>Southern California</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>#NewYearsResolution Finally master @ZJ10 's pa...</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution to stop being so damn perf...</td>\n",
              "      <td>Hollywood</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Philanthropic</td>\n",
              "      <td>My #NewYearsResolution is to help my disabled ...</td>\n",
              "      <td>Metro NYC</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution #2015Goals #2015bucketlist...</td>\n",
              "      <td>Pittsburgh, Pennsylvania</td>\n",
              "      <td>PA</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>female</td>\n",
              "      <td>Recreation &amp; Leisure</td>\n",
              "      <td>Tomorrow I start @JustifiedFX because @natalie...</td>\n",
              "      <td>NC/TN</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>holy crap, people. EYES OPEN WHEN DRIVING. #Ne...</td>\n",
              "      <td>charleston, nyc</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>RT @moJO_SHabby: Start parody of her blog #NYR...</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>female</td>\n",
              "      <td>Career</td>\n",
              "      <td>RT @kscmaghirang: To have an excellent job bef...</td>\n",
              "      <td>Paris  USA</td>\n",
              "      <td>TX</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>RT @tompycan: #NewYearsResolution on Jan1: \"I'...</td>\n",
              "      <td>shenandoah conservatory</td>\n",
              "      <td>VA</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5011 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      gender   Resolution_Category  ... tweet_state tweet_region\n",
              "0     female      Health & Fitness  ...          CA         West\n",
              "1     female                 Humor  ...          NJ    Northeast\n",
              "2       male       Personal Growth  ...          CA         West\n",
              "3       male         Philanthropic  ...          NY    Northeast\n",
              "4     female       Personal Growth  ...          PA    Northeast\n",
              "...      ...                   ...  ...         ...          ...\n",
              "5006  female  Recreation & Leisure  ...          TN        South\n",
              "5007  female                 Humor  ...          NY    Northeast\n",
              "5008  female                 Humor  ...          TN        South\n",
              "5009  female                Career  ...          TX        South\n",
              "5010  female      Health & Fitness  ...          VA        South\n",
              "\n",
              "[5011 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSgsArci3vNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R0NV5r7deWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Time for RegEx cleaning/processing.\n",
        "def cleaner(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub('b4', 'before', text)\n",
        "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
        "    text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0V0q1Llkq1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "b50c6769-96f0-410c-ba76-5325bbdfb403"
      },
      "source": [
        "#Cleaning all the text in our data with our newly build cleaner function.\n",
        "for i in range(len(first_clean.text)):\n",
        "  first_clean.text[i] = cleaner(first_clean.text[i])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Splitting the data into train and test set. + shuffling while doing so. the test size is = 20%.\n",
        "train, test = train_test_split(first_clean, random_state=42, test_size=0.2, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1973    Ã»Ã¯ chrisbrown supper club right now newyearsre...\n",
            "3666    rt goltv bae i want a man with a career i am o...\n",
            "831     newyearsresolution stop getting hit in the fac...\n",
            "239     newyearsresolution continue to date hilariousl...\n",
            "4692                    find more hoes newyearsresolution\n",
            "                              ...                        \n",
            "4426    i have newyearsresolution ideas amp really sho...\n",
            "466        newyearsresolution 1 give up chocolate for jan\n",
            "3092    new years resolution not to have a new years r...\n",
            "3772    i do not usually indulge in them but i have go...\n",
            "860               newyearsresolution stop burning barbies\n",
            "Name: text, Length: 4008, dtype: object\n",
            "\n",
            "1052    Ã»Ã¯ cinemax newyearsresolution be more badass h...\n",
            "3074    stuck with my new years resolution for 2 days ...\n",
            "1789    kalanisim55 be a badass as usual newyearsresol...\n",
            "764     rt rockii96 newyearsresolution go to the gym o...\n",
            "3822    feeling fat thinking about doing sprints amp e...\n",
            "                              ...                        \n",
            "3905    i can not wait to just be skinny and love my b...\n",
            "2942    rt hennynhandcuffs should i bother heading to ...\n",
            "4317    yes i am aware that i need more bacon in my di...\n",
            "3244    the start of 2015 my only newyearsresolution _...\n",
            "4376    i was gonna make my newyearsresolution about l...\n",
            "Name: text, Length: 1003, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bMi1oUjq8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a list of ALL tweets. (Each element in the list is a tweet)\n",
        "sequence = list()\n",
        "for i in range(len(cleaned_tweets)):\n",
        "  sequence.append(cleaned_tweets[i].strip())\n",
        "\n",
        "#Using Sklearn, playing around with the CountVectorizer.\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", lowercase=True, stop_words= \"english\",min_df= 5, binary= True)\n",
        "X = vectorizer.fit_transform(sequence)\n",
        "X.toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_.get(u\"newyearsresolution\"))\n",
        "print(X.toarray())\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VcXa5JjrCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#Here we use term frequency to downscale the importance of words occuring many times in a tweet. That way we hopefully get more weighted words. \n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
        "X_train_tfidf.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGlGbDejrHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, first_clean.Resolution_Category )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibg8tTh2jrMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-DaRS1NjrTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}