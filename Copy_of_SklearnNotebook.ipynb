{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SklearnNotebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpv2yDryRd/y7nrXawfGE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaspergroenbek98/first-first-year-project/blob/master/Copy_of_SklearnNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLhZtbJFjmmT",
        "colab_type": "code",
        "outputId": "3d4a2501-1b65-4b8b-a272-92bf1646d340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsbJDxBGjqxk",
        "colab_type": "code",
        "outputId": "d94a4772-2332-4ca3-9fb4-1a8e196598c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgeCy9_vYOZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "ac91c235-762b-440e-97e3-c0adb2962869"
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "first_clean = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "first_clean\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>Resolution_Category</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>tweet_state</th>\n",
              "      <th>tweet_region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>#NewYearsResolution :: Read more books, No scr...</td>\n",
              "      <td>Southern California</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>#NewYearsResolution Finally master @ZJ10 's pa...</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution to stop being so damn perf...</td>\n",
              "      <td>Hollywood</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Philanthropic</td>\n",
              "      <td>My #NewYearsResolution is to help my disabled ...</td>\n",
              "      <td>Metro NYC</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution #2015Goals #2015bucketlist...</td>\n",
              "      <td>Pittsburgh, Pennsylvania</td>\n",
              "      <td>PA</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>female</td>\n",
              "      <td>Recreation &amp; Leisure</td>\n",
              "      <td>Tomorrow I start @JustifiedFX because @natalie...</td>\n",
              "      <td>NC/TN</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>holy crap, people. EYES OPEN WHEN DRIVING. #Ne...</td>\n",
              "      <td>charleston, nyc</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>RT @moJO_SHabby: Start parody of her blog #NYR...</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>female</td>\n",
              "      <td>Career</td>\n",
              "      <td>RT @kscmaghirang: To have an excellent job bef...</td>\n",
              "      <td>Paris  USA</td>\n",
              "      <td>TX</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>RT @tompycan: #NewYearsResolution on Jan1: \"I'...</td>\n",
              "      <td>shenandoah conservatory</td>\n",
              "      <td>VA</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5011 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      gender   Resolution_Category  ... tweet_state tweet_region\n",
              "0     female      Health & Fitness  ...          CA         West\n",
              "1     female                 Humor  ...          NJ    Northeast\n",
              "2       male       Personal Growth  ...          CA         West\n",
              "3       male         Philanthropic  ...          NY    Northeast\n",
              "4     female       Personal Growth  ...          PA    Northeast\n",
              "...      ...                   ...  ...         ...          ...\n",
              "5006  female  Recreation & Leisure  ...          TN        South\n",
              "5007  female                 Humor  ...          NY    Northeast\n",
              "5008  female                 Humor  ...          TN        South\n",
              "5009  female                Career  ...          TX        South\n",
              "5010  female      Health & Fitness  ...          VA        South\n",
              "\n",
              "[5011 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R0NV5r7deWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bd051c9f-f27f-4fb4-ba19-fe359c1b8a6f"
      },
      "source": [
        "first_clean.Resolution_Category.values"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Health & Fitness', 'Humor', 'Personal Growth', ..., 'Humor',\n",
              "       'Career', 'Health & Fitness'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0V0q1Llkq1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A small statistic on comments and their category\n",
        "\n",
        "categories=list()\n",
        "for i in first_clean.Resolution_Category.values:\n",
        "  categories.append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bMi1oUjq8v",
        "colab_type": "code",
        "outputId": "b7562086-66fe-4d91-8bb0-d208f77942a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Create a list of ALL tweets. (Each element in the list is a tweet)\n",
        "sequence = list()\n",
        "for i in range(len(df.text)):\n",
        "  sequence.append(df.text[i].strip())\n",
        "\n",
        "#Using Sklearn, playing around with the CountVectorizer.\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", lowercase=True, stop_words= \"english\",min_df= 5, binary= True)\n",
        "X = vectorizer.fit_transform(sequence)\n",
        "X.toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_.get(u\"newyearsresolution\"))\n",
        "print(X.toarray())\n",
        "print(X.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['000', '10', '100', '1024', '1080p', '11', '12', '13', '14', '15', '17', '18', '1920x1080', '1st', '20', '200', '2013', '2014', '2015', '2015goals', '2016', '21', '24', '2k15', '2nd', '30', '365', '40', '50', '5sos', '__ù', '_ù', '_ù_ä', '_ùª', '_ùªî', '_ùó', '_ùô', '_ùô_', '_ùôî', '_ùõ', '_ùõø', 'able', 'abs', 'accept', 'accomplish', 'accomplished', 'account', 'achieve', 'act', 'acting', 'action', 'active', 'actually', 'add', 'adult', 'adventure', 'advice', 'afraid', 'age', 'ago', 'ahead', 'ain', 'alcohol', 'allow', 'alot', 'amazing', 'amp', 'amwriting', 'anybody', 'anymore', 'apartment', 'app', 'appreciate', 'arms', 'art', 'asap', 'asapferg', 'ask', 'asked', 'asking', 'ass', 'asshole', 'attend', 'attention', 'attitude', 'auntieannes', 'avoid', 'away', 'awesome', 'baby', 'bacon', 'bad', 'bae', 'bag', 'ball', 'balls', 'band', 'bar', 'baseball', 'based', 'battle', 'bc', 'beard', 'beat', 'beautiful', 'bed', 'beer', 'begin', 'believe', 'best', 'better', 'bible', 'big', 'bigger', 'birthday', 'bit', 'bitch', 'bitches', 'black', 'blair', 'blessed', 'block', 'blog', 'blogging', 'boat', 'body', 'book', 'books', 'boss', 'bought', 'boy', 'boyfriend', 'boys', 'break', 'breakfast', 'bring', 'bringing', 'broke', 'broken', 'brother', 'budget', 'build', 'burn', 'burning', 'business', 'busy', 'button', 'buy', 'buying', 'calendar', 'came', 'candy', 'car', 'cardio', 'care', 'career', 'cariloha', 'caring', 'carry', 'cat', 'catch', 'cats', 'cause', 'challenge', 'chance', 'chances', 'change', 'changes', 'changing', 'channel', 'check', 'chicago', 'chicken', 'children', 'chipotle', 'chocolate', 'choose', 'chorgrapher', 'chrisbrown', 'christ', 'christmas', 'chuck', 'church', 'cigarettes', 'clean', 'cleaning', 'clear', 'cliche', 'close', 'closer', 'clothes', 'club', 'coffee', 'coke', 'college', 'come', 'comes', 'coming', 'community', 'complaining', 'complete', 'completed', 'completely', 'concerts', 'confidence', 'confident', 'connected', 'consider', 'consume', 'content', 'continue', 'control', 'cook', 'cool', 'couch', 'couldn', 'count', 'country', 'couple', 'course', 'crack', 'crap', 'crazy', 'create', 'creative', 'criticism', 'crush', 'cursing', 'cut', 'cute', 'cuz', 'dad', 'daily', 'damn', 'dance', 'dancing', 'date', 'dates', 'day', 'days', 'debt', 'decided', 'decisions', 'dedicated', 'deep', 'definitely', 'degree', 'delsolcolor', 'deserve', 'desk', 'determined', 'dick', 'did', 'didn', 'die', 'diet', 'difference', 'different', 'direction', 'discover', 'does', 'doesn', 'dog', 'dogs', 'doing', 'don', 'dont', 'drama', 'draw', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drinks', 'drive', 'driving', 'drop', 'drunk', 'dslr', 'early', 'earth', 'easy', 'eat', 'eating', 'edm', 'effort', 'email', 'embrace', 'encouragement', 'end', 'energy', 'engage', 'enjoy', 'entire', 'episode', 'especially', 'eve', 'everybody', 'everyday', 'exactly', 'excited', 'exciting', 'excuses', 'exercise', 'exercising', 'expand', 'expect', 'face', 'facebook', 'fail', 'failed', 'faith', 'fake', 'fall', 'family', 'famous', 'far', 'fart', 'fast', 'fat', 'father', 'favorite', 'fb', 'fear', 'feel', 'feeling', 'feelings', 'fewer', 'fight', 'figure', 'film', 'finally', 'finding', 'finish', 'finished', 'fit', 'fitfam', 'fitness', 'focus', 'focused', 'folks', 'follow', 'followed', 'followers', 'following', 'food', 'foods', 'forget', 'forgive', 'forward', 'free', 'french', 'fresh', 'friend', 'friends', 'friendships', 'fuck', 'fucked', 'fucking', 'fucks', 'fun', 'funny', 'future', 'gain', 'game', 'games', 'gay', 'genuinely', 'getfit', 'gets', 'getting', 'gift', 'girl', 'girlfriend', 'girls', 'gives', 'giving', 'goal', 'goals', 'goalsfor2015', 'god', 'goes', 'going', 'gone', 'gonna', 'good', 'got', 'gotta', 'grades', 'graduate', 'grateful', 'great', 'ground', 'grow', 'growth', 'gt', 'guess', 'guitar', 'guy', 'guys', 'gym', 'habit', 'habits', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hands', 'hang', 'happen', 'happier', 'happiness', 'happy', 'happynewyear', 'hard', 'harder', 'hashtagoftheweek', 'hate', 'haters', 'hating', 'haven', 'having', 'head', 'health', 'healthier', 'healthy', 'hear', 'heart', 'hell', 'help', 'helping', 'hi', 'high', 'hit', 'hoes', 'hold', 'holding', 'holiday', 'holidays', 'home', 'homeless', 'hope', 'hopefully', 'hoping', 'hot', 'hour', 'hours', 'house', 'http', 'https', 'hug', 'human', 'hurt', 'husband', 'ice', 'idea', 'ideas', 'idk', 'im', 'ima', 'image', 'imma', 'important', 'improve', 'increase', 'inspiration', 'instagram', 'instead', 'intake', 'interesting', 'internship', 'isn', 'jack', 'jan', 'january', 'jesus', 'jk', 'job', 'john', 'join', 'joining', 'jokes', 'journal', 'joy', 'judge', 'judgmental', 'juice', 'jump', 'just', 'keeping', 'kick', 'kid', 'kidding', 'kids', 'kill', 'kind', 'kinder', 'kiss', 'know', 'l00tapp', 'l00tappp', 'late', 'laugh', 'lazy', 'lbs', 'learn', 'learned', 'learning', 'leave', 'left', 'legs', 'lessons', 'let', 'lets', 'letting', 'level', 'life', 'lifestyle', 'like', 'lil', 'limit', 'list', 'listen', 'listening', 'literally', 'little', 'live', 'lives', 'living', 'll', 'lmao', 'lmfao', 'lol', 'long', 'longer', 'look', 'looking', 'looks', 'loose', 'lord', 'lose', 'losing', 'loss', 'lost', 'lot', 'love', 'loved', 'loves', 'loving', 'low', 'lt', 'lying', 'madonna', 'make', 'makes', 'makeup', 'making', 'man', 'manage', 'marathon', 'march', 'marry', 'matter', 'maybe', 'mcdonald', 'meal', 'mean', 'means', 'meat', 'media', 'meet', 'membership', 'memories', 'mentally', 'midnight', 'mile', 'miles', 'mind', 'minutes', 'miss', 'mistakes', 'mix', 'mixtape', 'mlbpaclubhouse', 'mom', 'moment', 'mondayblogs', 'money', 'month', 'months', 'morning', 'motivated', 'motivation', 'motto', 'mouth', 'movie', 'movies', 'moving', 'mr', 'music', 'nashgrier', 'need', 'needs', 'negative', 'negativity', 'neighbors', 'netflix', 'new', 'newbeginnings', 'newyear', 'newyear2015', 'newyearnewme', 'newyearnewyou', 'newyears', 'newyearseve', 'newyearsresolution', 'nice', 'nicer', 'nicolewinhoffer', 'niggas', 'night', 'noticed', 'number', 'ny', 'nye', 'nye2015', 'nyr', 'officially', 'oh', 'ok', 'okay', 'old', 'ones', 'online', 'oops', 'open', 'opinion', 'opportunity', 'optimistic', 'order', 'organized', 'outside', 'pack', 'pants', 'paper', 'parents', 'parksandrecnbc', 'party', 'pass', 'passion', 'past', 'patience', 'pay', 'peace', 'people', 'perfect', 'person', 'personal', 'pet', 'pewdiepie', 'phone', 'photo', 'physically', 'pick', 'pics', 'pictures', 'pinterest', 'pizza', 'place', 'places', 'plan', 'planet', 'planning', 'plans', 'play', 'playing', 'plus', 'pointless', 'pop', 'popular', 'positive', 'possible', 'post', 'pounds', 'power', 'ppl', 'practice', 'pray', 'prayer', 'present', 'pretty', 'priority', 'probably', 'problem', 'problems', 'procrastinating', 'produce', 'productive', 'progress', 'promise', 'promised', 'promises', 'proud', 'pt', 'public', 'publish', 'purpose', 'push', 'putting', 'quality', 'quit', 'quitting', 'quote', 'raise', 'reach', 'read', 'reading', 'ready', 'real', 'realistic', 'realize', 'really', 'refer', 'refuse', 'relationship', 'relationships', 'remember', 'remove', 'resolution', 'resolutions', 'resolutionsfor2015', 'resolve', 'resolving', 'respect', 'responsible', 'rest', 'return', 'rich', 'rid', 'ride', 'right', 'road', 'rock', 'roll', 'room', 'rt', 'run', 'running', 'said', 'save', 'say', 'saying', 'says', 'school', 'science', 'screw', 'season', 'second', 'seeing', 'seen', 'self', 'selfie', 'selfies', 'send', 'series', 'seriously', 'set', 'setting', 'settle', 'sex', 'sh', 'shall', 'shape', 'share', 'sharing', 'shave', 'shit', 'shopping', 'shot', 'shows', 'sick', 'simple', 'simply', 'sing', 'single', 'sit', 'sitting', 'situation', 'skin', 'skinny', 'slap', 'sleep', 'sleeping', 'small', 'smile', 'smoke', 'smoking', 'sober', 'social', 'socialmedia', 'soda', 'somebody', 'son', 'song', 'songs', 'soon', 'sorry', 'sounds', 'spanish', 'speak', 'special', 'spend', 'spending', 'spent', 'spirit', 'sports', 'stand', 'starbucks', 'start', 'started', 'starting', 'starts', 'state', 'stay', 'staying', 'step', 'stepping', 'steps', 'stick', 'sticks', 'stop', 'store', 'story', 'straight', 'stress', 'strive', 'strong', 'stronger', 'stubhub', 'stuck', 'studio', 'stuff', 'stupid', 'succeed', 'success', 'successful', 'successfully', 'sugar', 'summer', 'super', 'supper', 'support', 'sure', 'surround', 'sweet', 'taco', 'tacos', 'taking', 'talk', 'talking', 'taste', 'tbh', 'teach', 'team', 'tell', 'telling', 'text', 'texting', 'thank', 'thankful', 'thanks', 'thats', 'theellenshow', 'thing', 'things', 'think', 'thinking', 'thought', 'thoughts', 'throw', 'tickets', 'till', 'time', 'times', 'tixwish', 'today', 'toilet', 'told', 'tolerance', 'tomorrow', 'tone', 'tonight', 'total', 'totally', 'touch', 'track', 'training', 'travel', 'treat', 'trips', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tweet', 'tweeting', 'tweets', 'twice', 'twitter', 'ugh', 'ugly', 'understanding', 'unplug', 'upset', 'use', 'used', 'using', 'vacation', 've', 'version', 'vibes', 'video', 'videos', 'visit', 'volunteer', 'vow', 'wait', 'waiting', 'wake', 'walk', 'wanna', 'want', 'wants', 'waste', 'watch', 'watching', 'water', 'way', 'ways', 'wear', 'wearing', 'wedding', 'weed', 'week', 'weeks', 'weight', 'weightloss', 'weird', 'went', 'white', 'wife', 'win', 'wine', 'winter', 'wish', 'wit', 'woman', 'women', 'won', 'word', 'words', 'work', 'working', 'workout', 'world', 'worry', 'worrying', 'worth', 'write', 'writing', 'wrong', 'wwf', 'ya', 'yall', 'yeah', 'year', 'years', 'yes', 'yesterday', 'yoga', 'york', 'youareworthit', 'youtube', 'zero', 'â_ù', 'ï_', 'ïî', 'ïó', 'û_', 'ûªm', 'ûªs', 'ûï']\n",
            "617\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(5011, 996)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VcXa5JjrCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0fa24592-98ec-4cfe-a00d-728a4c1872b7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
        "X_train_tfidf.shape\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5011, 996)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGlGbDejrHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "43d8d691-f7f2-495e-f714-4fe5ee8c7df1"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, first_clean.Resolution_Category )\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibg8tTh2jrMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-DaRS1NjrTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}