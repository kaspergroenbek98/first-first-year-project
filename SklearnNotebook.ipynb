{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SklearnNotebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjgB1KyF+noaFjrpQoXSOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaspergroenbek98/first-first-year-project/blob/master/SklearnNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLhZtbJFjmmT",
        "colab_type": "code",
        "outputId": "2976c6fa-ebc2-48eb-fe4f-a02cba1a692c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsbJDxBGjqxk",
        "colab_type": "code",
        "outputId": "15140cf7-5bb4-40fe-ab8e-fe8fe296d97f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgeCy9_vYOZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "b2398bee-9810-449c-8e83-1f8e2a6e1b87"
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "data = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "data\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>Resolution_Category</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>tweet_state</th>\n",
              "      <th>tweet_region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>#NewYearsResolution :: Read more books, No scr...</td>\n",
              "      <td>Southern California</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>#NewYearsResolution Finally master @ZJ10 's pa...</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution to stop being so damn perf...</td>\n",
              "      <td>Hollywood</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Philanthropic</td>\n",
              "      <td>My #NewYearsResolution is to help my disabled ...</td>\n",
              "      <td>Metro NYC</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>#NewYearsResolution #2015Goals #2015bucketlist...</td>\n",
              "      <td>Pittsburgh, Pennsylvania</td>\n",
              "      <td>PA</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>female</td>\n",
              "      <td>Recreation &amp; Leisure</td>\n",
              "      <td>Tomorrow I start @JustifiedFX because @natalie...</td>\n",
              "      <td>NC/TN</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>holy crap, people. EYES OPEN WHEN DRIVING. #Ne...</td>\n",
              "      <td>charleston, nyc</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>RT @moJO_SHabby: Start parody of her blog #NYR...</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>female</td>\n",
              "      <td>Career</td>\n",
              "      <td>RT @kscmaghirang: To have an excellent job bef...</td>\n",
              "      <td>Paris  USA</td>\n",
              "      <td>TX</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>RT @tompycan: #NewYearsResolution on Jan1: \"I'...</td>\n",
              "      <td>shenandoah conservatory</td>\n",
              "      <td>VA</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5011 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      gender   Resolution_Category  ... tweet_state tweet_region\n",
              "0     female      Health & Fitness  ...          CA         West\n",
              "1     female                 Humor  ...          NJ    Northeast\n",
              "2       male       Personal Growth  ...          CA         West\n",
              "3       male         Philanthropic  ...          NY    Northeast\n",
              "4     female       Personal Growth  ...          PA    Northeast\n",
              "...      ...                   ...  ...         ...          ...\n",
              "5006  female  Recreation & Leisure  ...          TN        South\n",
              "5007  female                 Humor  ...          NY    Northeast\n",
              "5008  female                 Humor  ...          TN        South\n",
              "5009  female                Career  ...          TX        South\n",
              "5010  female      Health & Fitness  ...          VA        South\n",
              "\n",
              "[5011 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSgsArci3vNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R0NV5r7deWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Time for RegEx cleaning/processing.\n",
        "def cleaner(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub('b4', 'before', text)\n",
        "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
        "    text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0V0q1Llkq1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning all the text in our data with our newly build cleaner function.\n",
        "for i in range(len(data.text)):\n",
        "  data.text[i] = cleaner(data.text[i])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Splitting the data into train and test set. + shuffling while doing so. the test size is = 20%.\n",
        "train, test = train_test_split(data, random_state=42, test_size=0.2, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bMi1oUjq8v",
        "colab_type": "code",
        "outputId": "fa2abf16-8edd-4330-95e1-a3233f17bf01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Using Sklearn, playing around with the CountVectorizer.\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", lowercase=True, stop_words= \"english\",min_df= 5, binary= True)\n",
        "#Create vectorized train set\n",
        "X_train = vectorizer.fit_transform(train.text)\n",
        "X_train.toarray()\n",
        "#Create vectorized test set\n",
        "X_test = vectorizer.transform(test.text)\n",
        "X_test.toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_.get(u\"newyearsresolution\"))\n",
        "print(X_train.toarray())\n",
        "print(X_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10', '100', '1080p', '11', '12', '14', '15', '17', '1920x1080', '1st', '20', '200', '2013', '2014', '2015', '2015goals', '2016', '21', '24', '2k15', '30', '365', '50', '__ù', '_ù', '_ù_ä', '_ùª', '_ùªî', '_ùó', '_ùô', '_ùô_', '_ùôî', '_ùõ', '_ùõø', 'able', 'abs', 'accept', 'accomplish', 'account', 'achieve', 'acting', 'action', 'active', 'actually', 'add', 'advice', 'afraid', 'age', 'ago', 'ai', 'alcohol', 'amazing', 'amp', 'amwriting', 'anybody', 'anymore', 'appreciate', 'arms', 'asap', 'asapferg', 'ask', 'asked', 'ass', 'asshole', 'attention', 'attitude', 'auntieannes', 'avoid', 'away', 'awesome', 'baby', 'bacon', 'bad', 'bae', 'ball', 'balls', 'band', 'bar', 'baseball', 'bc', 'beat', 'beautiful', 'bed', 'beer', 'begin', 'believe', 'best', 'better', 'bible', 'big', 'bigger', 'bit', 'bitch', 'bitches', 'black', 'blair', 'blessed', 'block', 'blog', 'blogging', 'boat', 'body', 'book', 'books', 'boss', 'boy', 'boyfriend', 'boys', 'break', 'breakfast', 'bring', 'bringing', 'broke', 'broken', 'budget', 'build', 'burn', 'burning', 'business', 'busy', 'buy', 'buying', 'calendar', 'candy', 'car', 'care', 'career', 'cariloha', 'caring', 'carry', 'catch', 'cats', 'cause', 'challenge', 'chances', 'change', 'changes', 'changing', 'channel', 'check', 'children', 'chipotle', 'chocolate', 'choose', 'chorgrapher', 'chrisbrown', 'christ', 'christmas', 'church', 'cigarettes', 'clean', 'cleaning', 'clear', 'close', 'closer', 'club', 'coffee', 'college', 'come', 'comes', 'coming', 'complaining', 'complete', 'completely', 'concerts', 'confidence', 'continue', 'control', 'cook', 'cool', 'count', 'country', 'couple', 'crack', 'crap', 'create', 'creative', 'criticism', 'crush', 'cut', 'cute', 'cuz', 'dad', 'daily', 'damn', 'dance', 'dancing', 'date', 'day', 'days', 'debt', 'decided', 'decisions', 'definitely', 'delsolcolor', 'deserve', 'did', 'die', 'diet', 'difference', 'different', 'direction', 'discover', 'does', 'dog', 'dogs', 'doing', 'dont', 'drama', 'draw', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drinks', 'drive', 'driving', 'drop', 'drunk', 'dslr', 'early', 'easy', 'eat', 'eating', 'effort', 'email', 'embrace', 'encouragement', 'end', 'energy', 'engage', 'enjoy', 'entire', 'especially', 'eve', 'everybody', 'everyday', 'excited', 'exciting', 'excuses', 'exercise', 'expand', 'face', 'facebook', 'fail', 'failed', 'faith', 'fake', 'fall', 'family', 'famous', 'far', 'fart', 'fast', 'fat', 'favorite', 'fb', 'fear', 'feel', 'feelings', 'fewer', 'fight', 'figure', 'film', 'finally', 'finish', 'fit', 'fitness', 'focus', 'folks', 'follow', 'followed', 'followers', 'following', 'food', 'foods', 'forget', 'forgive', 'forward', 'free', 'fresh', 'friend', 'friends', 'fuck', 'fucked', 'fucking', 'fucks', 'fun', 'future', 'gain', 'game', 'games', 'gay', 'genuinely', 'gets', 'getting', 'gift', 'girl', 'girlfriend', 'girls', 'gives', 'giving', 'goal', 'goals', 'goalsfor2015', 'god', 'goes', 'going', 'gonna', 'good', 'got', 'gotta', 'grades', 'graduate', 'grateful', 'great', 'grow', 'growth', 'gt', 'guess', 'guitar', 'guy', 'guys', 'gym', 'habit', 'haha', 'hahaha', 'hair', 'half', 'happen', 'happier', 'happiness', 'happy', 'happynewyear', 'hard', 'harder', 'hashtagoftheweek', 'hate', 'having', 'head', 'health', 'healthier', 'healthy', 'heart', 'hell', 'help', 'hi', 'high', 'hit', 'hold', 'holiday', 'holidays', 'home', 'homeless', 'hope', 'hopefully', 'hoping', 'hot', 'hour', 'hours', 'house', 'http', 'https', 'hug', 'human', 'hurt', 'idea', 'ideas', 'idk', 'im', 'ima', 'important', 'improve', 'increase', 'inspiration', 'instagram', 'instead', 'intake', 'jack', 'jan', 'january', 'jesus', 'jk', 'job', 'join', 'journal', 'joy', 'judgmental', 'juice', 'jump', 'just', 'keeping', 'kick', 'kidding', 'kids', 'kill', 'kind', 'kinder', 'know', 'l00tapp', 'late', 'laugh', 'lazy', 'lbs', 'learn', 'learned', 'learning', 'leave', 'left', 'legs', 'lessons', 'let', 'letting', 'level', 'life', 'lifestyle', 'like', 'limit', 'list', 'listen', 'listening', 'literally', 'little', 'live', 'lives', 'living', 'lmao', 'lmfao', 'lol', 'long', 'longer', 'look', 'looking', 'loose', 'lord', 'lose', 'losing', 'lost', 'lot', 'love', 'loving', 'madonna', 'make', 'makes', 'makeup', 'making', 'man', 'marathon', 'marry', 'matter', 'maybe', 'mean', 'means', 'meat', 'media', 'meet', 'memories', 'midnight', 'mile', 'miles', 'mind', 'minutes', 'miss', 'mixtape', 'mlbpaclubhouse', 'mom', 'moment', 'mondayblogs', 'money', 'month', 'months', 'morning', 'motivated', 'motivation', 'motto', 'mouth', 'movie', 'movies', 'moving', 'music', 'nashgrier', 'need', 'needs', 'negative', 'negativity', 'neighbors', 'netflix', 'new', 'newyear', 'newyear2015', 'newyearnewme', 'newyears', 'newyearseve', 'newyearsresolution', 'nice', 'nicer', 'nicolewinhoffer', 'niggas', 'night', 'noticed', 'number', 'ny', 'nye', 'nye2015', 'nyr', 'officially', 'oh', 'ok', 'okay', 'old', 'ones', 'oops', 'open', 'opinion', 'opportunity', 'optimistic', 'order', 'organized', 'outside', 'pack', 'pants', 'parents', 'party', 'past', 'patience', 'pay', 'peace', 'people', 'perfect', 'person', 'personal', 'pet', 'phone', 'photo', 'pick', 'pictures', 'pizza', 'place', 'places', 'plan', 'planet', 'planning', 'plans', 'play', 'pop', 'popular', 'positive', 'possible', 'post', 'pounds', 'power', 'ppl', 'practice', 'pray', 'prayer', 'present', 'pretty', 'priority', 'probably', 'problem', 'problems', 'procrastinating', 'produce', 'productive', 'promise', 'promised', 'proud', 'public', 'publish', 'putting', 'quality', 'quit', 'quote', 'raise', 'read', 'reading', 'ready', 'real', 'realistic', 'realize', 'really', 'refer', 'refuse', 'relationship', 'relationships', 'remember', 'resolution', 'resolutions', 'resolutionsfor2015', 'resolve', 'respect', 'rest', 'rich', 'rid', 'ride', 'right', 'road', 'room', 'rt', 'run', 'running', 'said', 'save', 'say', 'saying', 'says', 'school', 'science', 'season', 'second', 'seeing', 'seen', 'self', 'selfie', 'selfies', 'seriously', 'set', 'settle', 'shall', 'shape', 'share', 'shit', 'shopping', 'shot', 'shows', 'sick', 'simple', 'simply', 'single', 'sleep', 'small', 'smile', 'smoking', 'social', 'soda', 'son', 'song', 'soon', 'sounds', 'spanish', 'speak', 'special', 'spend', 'spending', 'spent', 'spirit', 'sports', 'starbucks', 'start', 'started', 'starting', 'starts', 'stay', 'step', 'stepping', 'steps', 'stick', 'sticks', 'stop', 'straight', 'stress', 'strong', 'stronger', 'stubhub', 'studio', 'stuff', 'stupid', 'success', 'successful', 'successfully', 'sugar', 'summer', 'super', 'supper', 'support', 'sure', 'surround', 'sweet', 'tacos', 'taking', 'talk', 'talking', 'taste', 'tbh', 'team', 'tell', 'thank', 'thanks', 'theellenshow', 'thing', 'things', 'think', 'thinking', 'thought', 'tickets', 'till', 'time', 'times', 'tixwish', 'today', 'told', 'tolerance', 'tomorrow', 'tone', 'tonight', 'totally', 'training', 'travel', 'treat', 'trips', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tweet', 'tweeting', 'tweets', 'twice', 'twitter', 'ugh', 'understanding', 'unplug', 'upset', 'use', 'used', 'using', 'vacation', 'video', 'videos', 'visit', 'vow', 'wait', 'waiting', 'wake', 'walk', 'wanna', 'want', 'wants', 'waste', 'watch', 'watching', 'water', 'way', 'wear', 'wearing', 'weed', 'week', 'weeks', 'weight', 'weightloss', 'weird', 'white', 'wife', 'win', 'wine', 'winter', 'wish', 'wit', 'wo', 'woman', 'women', 'word', 'words', 'work', 'working', 'workout', 'world', 'worry', 'worrying', 'worth', 'write', 'writing', 'wrong', 'wwf', 'ya', 'yall', 'yeah', 'year', 'years', 'yes', 'yesterday', 'yoga', 'york', 'youareworthit', 'youtube', 'zero', 'â_ù', 'ï_', 'ïî', 'ïó', 'û_', 'ûªm', 'ûªs', 'ûï']\n",
            "494\n",
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(4008, 797)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VcXa5JjrCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "66c1bcc7-9bc9-428e-ebb4-416eb642876a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#Here we use term frequency to downscale the importance of words occuring many times in a tweet. That way we hopefully get more weighted words. \n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
        "print(X_train_tfidf.shape)\n",
        "X_test_tfidf = tfidf_transformer.fit_transform(X_test)\n",
        "print(X_test_tfidf.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4008, 797)\n",
            "(1003, 797)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGlGbDejrHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NBmodel = MultinomialNB().fit(X_train, train.Resolution_Category)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibg8tTh2jrMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b75a07a8-381d-425e-b38d-591f8fb5c5a8"
      },
      "source": [
        "predicted = NBmodel.predict(X_test)\n",
        "np.mean(predicted == test.Resolution_Category)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5064805583250249"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-DaRS1NjrTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}