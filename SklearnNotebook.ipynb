{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SklearnNotebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaspergroenbek98/first-first-year-project/blob/master/SklearnNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLhZtbJFjmmT",
        "colab_type": "code",
        "outputId": "77c9cc39-8a6b-42a0-beee-d41b4834cb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsbJDxBGjqxk",
        "colab_type": "code",
        "outputId": "41d261e1-15a6-491c-a929-033a070d3584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Read the data into a header and a data np.array - the array is then shuffled\n",
        "with open('first-first-year-project/data.csv', encoding='latin1') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    data = np.array([np.array(line) for line in csv_reader])\n",
        "header, OGdata = data[0,:], data[1:,:]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bMi1oUjq8v",
        "colab_type": "code",
        "outputId": "1517472a-20e0-495a-f977-7bb7bd543c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#Create a list of ALL tweets. (Each element in the list is a tweet)\n",
        "sequence = list()\n",
        "for i in range(len(OGdata[:,6])):\n",
        "  sequence.append(OGdata[i,6].strip())\n",
        "\n",
        "#Using Sklearn, playing around with the CountVectorizer.\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", lowercase=True, stop_words= \"english\",min_df= 5, binary= True)\n",
        "X = vectorizer.fit_transform(sequence)\n",
        "X.toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_.get(u\"newyearsresolution\"))\n",
        "print(X.toarray())\n",
        "print(X.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['000', '10', '100', '1024', '1080p', '11', '12', '13', '14', '15', '17', '18', '1920x1080', '1st', '20', '200', '2013', '2014', '2015', '2015goals', '2016', '21', '24', '2k15', '2nd', '30', '365', '40', '50', '5sos', '__ù', '_ù', '_ù_ä', '_ùª', '_ùªî', '_ùó', '_ùô', '_ùô_', '_ùôî', '_ùõ', '_ùõø', 'able', 'abs', 'accept', 'accomplish', 'accomplished', 'account', 'achieve', 'act', 'acting', 'action', 'active', 'actually', 'add', 'adult', 'adventure', 'advice', 'afraid', 'age', 'ago', 'ahead', 'ain', 'alcohol', 'allow', 'alot', 'amazing', 'amp', 'amwriting', 'anybody', 'anymore', 'apartment', 'app', 'appreciate', 'arms', 'art', 'asap', 'asapferg', 'ask', 'asked', 'asking', 'ass', 'asshole', 'attend', 'attention', 'attitude', 'auntieannes', 'avoid', 'away', 'awesome', 'baby', 'bacon', 'bad', 'bae', 'bag', 'ball', 'balls', 'band', 'bar', 'baseball', 'based', 'battle', 'bc', 'beard', 'beat', 'beautiful', 'bed', 'beer', 'begin', 'believe', 'best', 'better', 'bible', 'big', 'bigger', 'birthday', 'bit', 'bitch', 'bitches', 'black', 'blair', 'blessed', 'block', 'blog', 'blogging', 'boat', 'body', 'book', 'books', 'boss', 'bought', 'boy', 'boyfriend', 'boys', 'break', 'breakfast', 'bring', 'bringing', 'broke', 'broken', 'brother', 'budget', 'build', 'burn', 'burning', 'business', 'busy', 'button', 'buy', 'buying', 'calendar', 'came', 'candy', 'car', 'cardio', 'care', 'career', 'cariloha', 'caring', 'carry', 'cat', 'catch', 'cats', 'cause', 'challenge', 'chance', 'chances', 'change', 'changes', 'changing', 'channel', 'check', 'chicago', 'chicken', 'children', 'chipotle', 'chocolate', 'choose', 'chorgrapher', 'chrisbrown', 'christ', 'christmas', 'chuck', 'church', 'cigarettes', 'clean', 'cleaning', 'clear', 'cliche', 'close', 'closer', 'clothes', 'club', 'coffee', 'coke', 'college', 'come', 'comes', 'coming', 'community', 'complaining', 'complete', 'completed', 'completely', 'concerts', 'confidence', 'confident', 'connected', 'consider', 'consume', 'content', 'continue', 'control', 'cook', 'cool', 'couch', 'couldn', 'count', 'country', 'couple', 'course', 'crack', 'crap', 'crazy', 'create', 'creative', 'criticism', 'crush', 'cursing', 'cut', 'cute', 'cuz', 'dad', 'daily', 'damn', 'dance', 'dancing', 'date', 'dates', 'day', 'days', 'debt', 'decided', 'decisions', 'dedicated', 'deep', 'definitely', 'degree', 'delsolcolor', 'deserve', 'desk', 'determined', 'dick', 'did', 'didn', 'die', 'diet', 'difference', 'different', 'direction', 'discover', 'does', 'doesn', 'dog', 'dogs', 'doing', 'don', 'dont', 'drama', 'draw', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drinks', 'drive', 'driving', 'drop', 'drunk', 'dslr', 'early', 'earth', 'easy', 'eat', 'eating', 'edm', 'effort', 'email', 'embrace', 'encouragement', 'end', 'energy', 'engage', 'enjoy', 'entire', 'episode', 'especially', 'eve', 'everybody', 'everyday', 'exactly', 'excited', 'exciting', 'excuses', 'exercise', 'exercising', 'expand', 'expect', 'face', 'facebook', 'fail', 'failed', 'faith', 'fake', 'fall', 'family', 'famous', 'far', 'fart', 'fast', 'fat', 'father', 'favorite', 'fb', 'fear', 'feel', 'feeling', 'feelings', 'fewer', 'fight', 'figure', 'film', 'finally', 'finding', 'finish', 'finished', 'fit', 'fitfam', 'fitness', 'focus', 'focused', 'folks', 'follow', 'followed', 'followers', 'following', 'food', 'foods', 'forget', 'forgive', 'forward', 'free', 'french', 'fresh', 'friend', 'friends', 'friendships', 'fuck', 'fucked', 'fucking', 'fucks', 'fun', 'funny', 'future', 'gain', 'game', 'games', 'gay', 'genuinely', 'getfit', 'gets', 'getting', 'gift', 'girl', 'girlfriend', 'girls', 'gives', 'giving', 'goal', 'goals', 'goalsfor2015', 'god', 'goes', 'going', 'gone', 'gonna', 'good', 'got', 'gotta', 'grades', 'graduate', 'grateful', 'great', 'ground', 'grow', 'growth', 'gt', 'guess', 'guitar', 'guy', 'guys', 'gym', 'habit', 'habits', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hands', 'hang', 'happen', 'happier', 'happiness', 'happy', 'happynewyear', 'hard', 'harder', 'hashtagoftheweek', 'hate', 'haters', 'hating', 'haven', 'having', 'head', 'health', 'healthier', 'healthy', 'hear', 'heart', 'hell', 'help', 'helping', 'hi', 'high', 'hit', 'hoes', 'hold', 'holding', 'holiday', 'holidays', 'home', 'homeless', 'hope', 'hopefully', 'hoping', 'hot', 'hour', 'hours', 'house', 'http', 'https', 'hug', 'human', 'hurt', 'husband', 'ice', 'idea', 'ideas', 'idk', 'im', 'ima', 'image', 'imma', 'important', 'improve', 'increase', 'inspiration', 'instagram', 'instead', 'intake', 'interesting', 'internship', 'isn', 'jack', 'jan', 'january', 'jesus', 'jk', 'job', 'john', 'join', 'joining', 'jokes', 'journal', 'joy', 'judge', 'judgmental', 'juice', 'jump', 'just', 'keeping', 'kick', 'kid', 'kidding', 'kids', 'kill', 'kind', 'kinder', 'kiss', 'know', 'l00tapp', 'l00tappp', 'late', 'laugh', 'lazy', 'lbs', 'learn', 'learned', 'learning', 'leave', 'left', 'legs', 'lessons', 'let', 'lets', 'letting', 'level', 'life', 'lifestyle', 'like', 'lil', 'limit', 'list', 'listen', 'listening', 'literally', 'little', 'live', 'lives', 'living', 'll', 'lmao', 'lmfao', 'lol', 'long', 'longer', 'look', 'looking', 'looks', 'loose', 'lord', 'lose', 'losing', 'loss', 'lost', 'lot', 'love', 'loved', 'loves', 'loving', 'low', 'lt', 'lying', 'madonna', 'make', 'makes', 'makeup', 'making', 'man', 'manage', 'marathon', 'march', 'marry', 'matter', 'maybe', 'mcdonald', 'meal', 'mean', 'means', 'meat', 'media', 'meet', 'membership', 'memories', 'mentally', 'midnight', 'mile', 'miles', 'mind', 'minutes', 'miss', 'mistakes', 'mix', 'mixtape', 'mlbpaclubhouse', 'mom', 'moment', 'mondayblogs', 'money', 'month', 'months', 'morning', 'motivated', 'motivation', 'motto', 'mouth', 'movie', 'movies', 'moving', 'mr', 'music', 'nashgrier', 'need', 'needs', 'negative', 'negativity', 'neighbors', 'netflix', 'new', 'newbeginnings', 'newyear', 'newyear2015', 'newyearnewme', 'newyearnewyou', 'newyears', 'newyearseve', 'newyearsresolution', 'nice', 'nicer', 'nicolewinhoffer', 'niggas', 'night', 'noticed', 'number', 'ny', 'nye', 'nye2015', 'nyr', 'officially', 'oh', 'ok', 'okay', 'old', 'ones', 'online', 'oops', 'open', 'opinion', 'opportunity', 'optimistic', 'order', 'organized', 'outside', 'pack', 'pants', 'paper', 'parents', 'parksandrecnbc', 'party', 'pass', 'passion', 'past', 'patience', 'pay', 'peace', 'people', 'perfect', 'person', 'personal', 'pet', 'pewdiepie', 'phone', 'photo', 'physically', 'pick', 'pics', 'pictures', 'pinterest', 'pizza', 'place', 'places', 'plan', 'planet', 'planning', 'plans', 'play', 'playing', 'plus', 'pointless', 'pop', 'popular', 'positive', 'possible', 'post', 'pounds', 'power', 'ppl', 'practice', 'pray', 'prayer', 'present', 'pretty', 'priority', 'probably', 'problem', 'problems', 'procrastinating', 'produce', 'productive', 'progress', 'promise', 'promised', 'promises', 'proud', 'pt', 'public', 'publish', 'purpose', 'push', 'putting', 'quality', 'quit', 'quitting', 'quote', 'raise', 'reach', 'read', 'reading', 'ready', 'real', 'realistic', 'realize', 'really', 'refer', 'refuse', 'relationship', 'relationships', 'remember', 'remove', 'resolution', 'resolutions', 'resolutionsfor2015', 'resolve', 'resolving', 'respect', 'responsible', 'rest', 'return', 'rich', 'rid', 'ride', 'right', 'road', 'rock', 'roll', 'room', 'rt', 'run', 'running', 'said', 'save', 'say', 'saying', 'says', 'school', 'science', 'screw', 'season', 'second', 'seeing', 'seen', 'self', 'selfie', 'selfies', 'send', 'series', 'seriously', 'set', 'setting', 'settle', 'sex', 'sh', 'shall', 'shape', 'share', 'sharing', 'shave', 'shit', 'shopping', 'shot', 'shows', 'sick', 'simple', 'simply', 'sing', 'single', 'sit', 'sitting', 'situation', 'skin', 'skinny', 'slap', 'sleep', 'sleeping', 'small', 'smile', 'smoke', 'smoking', 'sober', 'social', 'socialmedia', 'soda', 'somebody', 'son', 'song', 'songs', 'soon', 'sorry', 'sounds', 'spanish', 'speak', 'special', 'spend', 'spending', 'spent', 'spirit', 'sports', 'stand', 'starbucks', 'start', 'started', 'starting', 'starts', 'state', 'stay', 'staying', 'step', 'stepping', 'steps', 'stick', 'sticks', 'stop', 'store', 'story', 'straight', 'stress', 'strive', 'strong', 'stronger', 'stubhub', 'stuck', 'studio', 'stuff', 'stupid', 'succeed', 'success', 'successful', 'successfully', 'sugar', 'summer', 'super', 'supper', 'support', 'sure', 'surround', 'sweet', 'taco', 'tacos', 'taking', 'talk', 'talking', 'taste', 'tbh', 'teach', 'team', 'tell', 'telling', 'text', 'texting', 'thank', 'thankful', 'thanks', 'thats', 'theellenshow', 'thing', 'things', 'think', 'thinking', 'thought', 'thoughts', 'throw', 'tickets', 'till', 'time', 'times', 'tixwish', 'today', 'toilet', 'told', 'tolerance', 'tomorrow', 'tone', 'tonight', 'total', 'totally', 'touch', 'track', 'training', 'travel', 'treat', 'trips', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tweet', 'tweeting', 'tweets', 'twice', 'twitter', 'ugh', 'ugly', 'understanding', 'unplug', 'upset', 'use', 'used', 'using', 'vacation', 've', 'version', 'vibes', 'video', 'videos', 'visit', 'volunteer', 'vow', 'wait', 'waiting', 'wake', 'walk', 'wanna', 'want', 'wants', 'waste', 'watch', 'watching', 'water', 'way', 'ways', 'wear', 'wearing', 'wedding', 'weed', 'week', 'weeks', 'weight', 'weightloss', 'weird', 'went', 'white', 'wife', 'win', 'wine', 'winter', 'wish', 'wit', 'woman', 'women', 'won', 'word', 'words', 'work', 'working', 'workout', 'world', 'worry', 'worrying', 'worth', 'write', 'writing', 'wrong', 'wwf', 'ya', 'yall', 'yeah', 'year', 'years', 'yes', 'yesterday', 'yoga', 'york', 'youareworthit', 'youtube', 'zero', 'â_ù', 'ï_', 'ïî', 'ïó', 'û_', 'ûªm', 'ûªs', 'ûï']\n",
            "617\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(5011, 996)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHaPGn3C8wsQ",
        "colab_type": "text"
      },
      "source": [
        "# KFold playground\n",
        "### The code below is not finished and thus doesn't work at this time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi2Xv0NdMscz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "47807f32-219f-471c-a3ff-bbc09f0dfb31"
      },
      "source": [
        "kf = KFold(n_splits=3)\n",
        "sum = 0\n",
        "for train, test in kf.split(sequence):\n",
        "    train_data = np.array(sequence)[train]\n",
        "    test_data = np.array(sequence)[test]\n",
        "    classifier = nltk.NaiveBayesClassifier(train_data)\n",
        "    sum += nltk.classify.accuracy(classifier, test_data)\n",
        "average = sum/3"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-3a94e8cb8c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'feature_probdist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VcXa5JjrCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Preperation) imports kfold model and initializes a variable as an object of the kfold class\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(5, True, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGlGbDejrHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "85ecdf9e-21e8-4857-fc89-669a706da494"
      },
      "source": [
        "# This chunk enumerates the k-amount of splits\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn import metrics\n",
        "from sklearn import linear_model\n",
        "\n",
        "lm = linear_model.LinearRegression()\n",
        "\n",
        "for train, test in kf.split(data):\n",
        "  \n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-6ac71fb0646e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index (5011) out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OuU2hd_HYN-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}