{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg7izubCnhOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "85ab9ffa-cecd-48c6-a2a9-45e1fbe1db16"
      },
      "source": [
        "### Functions\n",
        "\n",
        "### START OF VOCABULARY ###\n",
        "def generate_vocabulary(data, fCol, vocabType):\n",
        "    '''\n",
        "    Returns a list/vocabulary of len <= \"size\" based on the vocabType and the featureColumn specified\n",
        "    '''\n",
        "    size = 2000\n",
        "    # Only get large groups to get representative data\n",
        "    major_features = np.array(nltk.FreqDist(data[:,fCol]).most_common(5))\n",
        "    major_masks = np.array([data[:,fCol] == f for (f, cnt) in major_features])\n",
        "    fqs = [tweet_word_distribution(data[mask,:]) for mask in major_masks]\n",
        "    return list(vocabType(data, fCol, major_features, major_masks, fqs, size))\n",
        "\n",
        "def vocab_feature_most_common(data, fCol, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises the most common words for each feature\n",
        "    '''\n",
        "    vocabulary = set()\n",
        "    # get the most common words in each freq dist. zip(*...) removes the counts from fd, and updates vocabulary ONLY with the words\n",
        "    for fd in fqs:\n",
        "        vocabulary.update(list(zip(*fd.most_common(size//len(major_features))))[0])\n",
        "    return vocabulary\n",
        "\n",
        "def vocab_feature_unique(data, fCol, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises words which are uncommon in other features, but common in one feature\n",
        "    '''\n",
        "    major_mask = np.array(major_masks.sum(axis=0), dtype='bool')\n",
        "    fq = tweet_word_distribution(data[major_mask,6]) # Get a fq over the words used by all in the major categories\n",
        "    words = [word for (word, cnt) in fq.items() if cnt >= 20] # removes rarely mentioned words which probably arent indicative of a significant trend\n",
        "    priorityArray = []\n",
        "    for i, word in enumerate(words):\n",
        "        priorityArray.append([word])\n",
        "        #divide frequency of word in that state by the tweetcount from that state, and by how often that word is used in total by all states\n",
        "        score = max(fqs[fID][word]/(int(major_features[fID][1])*fq[word]) for fID in range(len(major_masks)))\n",
        "        priorityArray[i].append(score)\n",
        "    priorityArray.sort(key = lambda x: x[1], reverse=True) # Sort them based on their best score\n",
        "    vocabulary = list(zip(*priorityArray[:size]))[0] # Removes their scores\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def tweet_word_distribution(data):\n",
        "    # Split each sentence into tokens, and create a frequency distribution\n",
        "    tokens = [token for sentence in data[:,6] for token in sentence.split()]\n",
        "    fd = nltk.FreqDist(tokens)\n",
        "    return fd\n",
        "\n",
        "### END OF VOCABULARY ###\n",
        "    \n",
        "\n",
        "# def identify_hashtags(data):\n",
        "#     col = np.zeros((data.shape[0],1), 'str')\n",
        "#     data = np.append(data, col, axis=1)\n",
        "#     for i, text in enumerate(data[:,6]):\n",
        "#         results = re.findall(r\"#\\w+\", text) # Finds matches and returns them as an iterable\n",
        "#         if results:\n",
        "#             data[i,15] = ' '.join(results)\n",
        "#         else:\n",
        "#             data[i,15] = ''\n",
        "\n",
        "\n",
        "def divide_featureset(feature):\n",
        "    '''\n",
        "    Divides numpy featureset of (featureVector, classification) into a 80:10:10 train:dev:test set\n",
        "    '''\n",
        "    testSize = int(len(feature)*0.8)\n",
        "    train, rest = feature[:testSize], feature[testSize:]\n",
        "    restSize = len(rest)//2\n",
        "    dev, test = rest[:restSize], rest[restSize:]\n",
        "    return train, dev, test\n",
        "\n",
        "\n",
        "#RegEx cleaner for SKlearn (Maybe merge with Clean()).\n",
        "def cleaner(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub('b4', 'before', text)\n",
        "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
        "    text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
        "    text = text.strip(' ')\n",
        "    # for t in text:\n",
        "    #   text = ''.join(char for char in tweet if char.isalpha() or char == ' ')\n",
        "    text = ' '.join(stemmer.stem(text))\n",
        "    return text\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Needs revisiting (below)\n",
        "\"\"\"\n",
        "# def clean(tweet):\n",
        "#     stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "#     # Remove all stopwords, non-alphabet words (except spaces), and stem the words\n",
        "#     for text in tweet:\n",
        "#     text = text.lower()\n",
        "#     text = ''.join(char for char in tweet if char.isalpha() or char == ' ')\n",
        "#     text = ' '.join(stemmer.stem(text))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Needs a concrete decision on future use of the code below\n",
        "\"\"\"\n",
        "# def generate_features(data, vocabulary, fCol):\n",
        "#     \"\"\" \n",
        "#     Creates tuples with a vector containing boolean values depending on whether\n",
        "#     or not the word is in the tweet - along with the label of the tweet.\n",
        "#     \"\"\"\n",
        "#     features = [(tweet_features(d.split(), vocabulary), c) for (d,c) in zip(data[:,6], data[:,fCol])] # column 6 is text data, column 2 is gender data\n",
        "#     return features"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-6bfd0ed259ab>\"\u001b[0;36m, line \u001b[0;32m106\u001b[0m\n\u001b[0;31m    text = text.lower()\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV3IQMyQnWOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "364ed109-4efb-48bb-a81f-213300866bd9"
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n",
        "### All import calls.\n",
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'first-first-year-project'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 116 (delta 58), reused 21 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (116/116), 600.36 KiB | 14.29 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgs7KbInxrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8d23b1d-63c9-4836-de5c-16f17ba2ad60"
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "data = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "#Cleaning all the text in our data with our newly build cleaner() function.\n",
        "for i in range(len(data.text)):\n",
        "  data.text[i] = cleaner(data.text[i])\n",
        "\n",
        "data.text[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'newyearsresolution finally master zj10 part of kitchen sink'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEtMB4V7tuWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}