{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg7izubCnhOn",
        "colab_type": "code",
        "outputId": "8f464984-9e70-4aff-92ed-88a813ac21e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "### Functions\n",
        "\n",
        "### START OF VOCABULARY ###\n",
        "def generate_vocabulary(data, fCol, vocabType):\n",
        "    '''\n",
        "    Returns a list/vocabulary of len <= \"size\" based on the vocabType and the featureColumn specified\n",
        "    '''\n",
        "    size = 2000\n",
        "    # Only get large groups to get representative data\n",
        "    major_features = np.array(nltk.FreqDist(data[:,fCol]).most_common(5))\n",
        "    major_masks = np.array([data[:,fCol] == f for (f, cnt) in major_features])\n",
        "    fqs = [tweet_word_distribution(data[mask,:]) for mask in major_masks]\n",
        "    return list(vocabType(data, fCol, major_features, major_masks, fqs, size))\n",
        "\n",
        "def vocab_feature_most_common(data, fCol, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises the most common words for each feature\n",
        "    '''\n",
        "    vocabulary = set()\n",
        "    # get the most common words in each freq dist. zip(*...) removes the counts from fd, and updates vocabulary ONLY with the words\n",
        "    for fd in fqs:\n",
        "        vocabulary.update(list(zip(*fd.most_common(size//len(major_features))))[0])\n",
        "    return vocabulary\n",
        "\n",
        "def vocab_feature_unique(data, fCol, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises words which are uncommon in other features, but common in one feature\n",
        "    '''\n",
        "    major_mask = np.array(major_masks.sum(axis=0), dtype='bool')\n",
        "    fq = tweet_word_distribution(data[major_mask,6]) # Get a fq over the words used by all in the major categories\n",
        "    words = [word for (word, cnt) in fq.items() if cnt >= 20] # removes rarely mentioned words which probably arent indicative of a significant trend\n",
        "    priorityArray = []\n",
        "    for i, word in enumerate(words):\n",
        "        priorityArray.append([word])\n",
        "        #divide frequency of word in that state by the tweetcount from that state, and by how often that word is used in total by all states\n",
        "        score = max(fqs[fID][word]/(int(major_features[fID][1])*fq[word]) for fID in range(len(major_masks)))\n",
        "        priorityArray[i].append(score)\n",
        "    priorityArray.sort(key = lambda x: x[1], reverse=True) # Sort them based on their best score\n",
        "    vocabulary = list(zip(*priorityArray[:size]))[0] # Removes their scores\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def tweet_word_distribution(data):\n",
        "    # Split each sentence into tokens, and create a frequency distribution\n",
        "    tokens = [token for sentence in data[:,6] for token in sentence.split()]\n",
        "    fd = nltk.FreqDist(tokens)\n",
        "    return fd\n",
        "\n",
        "### END OF VOCABULARY ###\n",
        "    \n",
        "\n",
        "# def identify_hashtags(data):\n",
        "#     col = np.zeros((data.shape[0],1), 'str')\n",
        "#     data = np.append(data, col, axis=1)\n",
        "#     for i, text in enumerate(data[:,6]):\n",
        "#         results = re.findall(r\"#\\w+\", text) # Finds matches and returns them as an iterable\n",
        "#         if results:\n",
        "#             data[i,15] = ' '.join(results)\n",
        "#         else:\n",
        "#             data[i,15] = ''\n",
        "\n",
        "\n",
        "def divide_featureset(feature):\n",
        "    '''\n",
        "    Divides numpy featureset of (featureVector, classification) into a 80:10:10 train:dev:test set\n",
        "    '''\n",
        "    testSize = int(len(feature)*0.8)\n",
        "    train, rest = feature[:testSize], feature[testSize:]\n",
        "    restSize = len(rest)//2\n",
        "    dev, test = rest[:restSize], rest[restSize:]\n",
        "    return train, dev, test\n",
        "\n",
        "\n",
        "#RegEx cleaner for SKlearn (Maybe merge with Clean()).\n",
        "def cleaner(text):\n",
        "  stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "  text = text.lower() #All lowercase\n",
        "  text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
        "  text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
        "  text = re.sub(r\"what's\", \"what is \", text)\n",
        "  text = re.sub(r\"\\'s\", \" \", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "  text = re.sub(r\"can't\", \"can not \", text)\n",
        "  text = re.sub(r\"n't\", \" not \", text)\n",
        "  text = re.sub(r\"i'm\", \"i am \", text)\n",
        "  text = re.sub(r\"\\'re\", \" are \", text)\n",
        "  text = re.sub(r\"\\'d\", \" would \", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "  text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "  text = re.sub('\\W', ' ', text)\n",
        "  text = re.sub('\\s+', ' ', text)\n",
        "  text = re.sub('b4', 'before', text)\n",
        "  text = text.strip(' ')\n",
        "  text = ''.join(char for char in text if char.isalpha() or char == ' ')\n",
        "  text = ''.join(stemmer.stem(text)) #Stem the words using SnowballStemmer\n",
        "  return text\n",
        "\n",
        "def delete_rows(dataset, column, ID):\n",
        "  \"\"\"\n",
        "  Takes a panda dataframe as first arg. The df column as second arg and the ID \n",
        "  as third. Returns a mask shoving only those values in the dataset.\n",
        "  \"\"\"\n",
        "  mask = dataset[dataset[str(column)] == str(ID)]\n",
        "  return mask\n",
        "\n",
        "\"\"\"\n",
        "Needs revisiting (below)\n",
        "\"\"\"\n",
        "# def clean(tweet):\n",
        "#     stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "#     # Remove all stopwords, non-alphabet words (except spaces), and stem the words\n",
        "#     for text in tweet:\n",
        "#     text = text.lower()\n",
        "#     text = ''.join(char for char in tweet if char.isalpha() or char == ' ')\n",
        "#     text = ' '.join(stemmer.stem(text))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Needs a concrete decision on future use of the code below\n",
        "\"\"\"\n",
        "# def generate_features(data, vocabulary, fCol):\n",
        "#     \"\"\" \n",
        "#     Creates tuples with a vector containing boolean values depending on whether\n",
        "#     or not the word is in the tweet - along with the label of the tweet.\n",
        "#     \"\"\"\n",
        "#     features = [(tweet_features(d.split(), vocabulary), c) for (d,c) in zip(data[:,6], data[:,fCol])] # column 6 is text data, column 2 is gender data\n",
        "#     return features"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNeeds a concrete decision on future use of the code below\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV3IQMyQnWOs",
        "colab_type": "code",
        "outputId": "0b0ee7b2-1484-4ff0-f5d6-a0fab7feabfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n",
        "### All import calls.\n",
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgs7KbInxrx",
        "colab_type": "code",
        "outputId": "9a5d552f-7690-4e32-f209-534ba3c17600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "data = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "\n",
        "\n",
        "#Cleaning all the text in our data with our newly build cleaner() function.\n",
        "for i in range(len(data.text)):\n",
        "  data.text[i] = cleaner(data.text[i])\n",
        "\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>Resolution_Category</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>tweet_state</th>\n",
              "      <th>tweet_region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>newyearsresolution read more books no scrollin...</td>\n",
              "      <td>Southern California</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>newyearsresolution finally master part of kitc...</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>newyearsresolution to stop being so damn perf ...</td>\n",
              "      <td>Hollywood</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Philanthropic</td>\n",
              "      <td>my newyearsresolution is to help my disabled p...</td>\n",
              "      <td>Metro NYC</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>newyearsresolution goals bucketlist continued  ü</td>\n",
              "      <td>Pittsburgh, Pennsylvania</td>\n",
              "      <td>PA</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>female</td>\n",
              "      <td>Recreation &amp; Leisure</td>\n",
              "      <td>tomorrow i start because and it comes highly r...</td>\n",
              "      <td>NC/TN</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>holy crap people eyes open when driving newyea...</td>\n",
              "      <td>charleston, nyc</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>rt shabby start parody of her blog nyresolutio...</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>female</td>\n",
              "      <td>Career</td>\n",
              "      <td>rt to have an excellent job before or after gr...</td>\n",
              "      <td>Paris  USA</td>\n",
              "      <td>TX</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>rt newyearsresolution on jan i am really going...</td>\n",
              "      <td>shenandoah conservatory</td>\n",
              "      <td>VA</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5011 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      gender   Resolution_Category  ... tweet_state tweet_region\n",
              "0     female      Health & Fitness  ...          CA         West\n",
              "1     female                 Humor  ...          NJ    Northeast\n",
              "2       male       Personal Growth  ...          CA         West\n",
              "3       male         Philanthropic  ...          NY    Northeast\n",
              "4     female       Personal Growth  ...          PA    Northeast\n",
              "...      ...                   ...  ...         ...          ...\n",
              "5006  female  Recreation & Leisure  ...          TN        South\n",
              "5007  female                 Humor  ...          NY    Northeast\n",
              "5008  female                 Humor  ...          TN        South\n",
              "5009  female                Career  ...          TX        South\n",
              "5010  female      Health & Fitness  ...          VA        South\n",
              "\n",
              "[5011 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnjnqpS6eIAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "f36ebff6-8dfc-4193-d333-c0acbaa38103"
      },
      "source": [
        "#Data sets \n",
        "res_data = 1\n",
        "\n",
        "newD = delete_rows(data, \"gender\", \"female\")\n",
        "newD"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>Resolution_Category</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>tweet_state</th>\n",
              "      <th>tweet_region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>newyearsresolution read more books no scrollin...</td>\n",
              "      <td>Southern California</td>\n",
              "      <td>CA</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>newyearsresolution finally master part of kitc...</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>newyearsresolution goals bucketlist continued  ü</td>\n",
              "      <td>Pittsburgh, Pennsylvania</td>\n",
              "      <td>PA</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>rt easy newyearsresolution stand up for your h...</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>female</td>\n",
              "      <td>Personal Growth</td>\n",
              "      <td>yes let all take calculated risks and reduce s...</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>female</td>\n",
              "      <td>Recreation &amp; Leisure</td>\n",
              "      <td>tomorrow i start because and it comes highly r...</td>\n",
              "      <td>NC/TN</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>holy crap people eyes open when driving newyea...</td>\n",
              "      <td>charleston, nyc</td>\n",
              "      <td>NY</td>\n",
              "      <td>Northeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>female</td>\n",
              "      <td>Humor</td>\n",
              "      <td>rt shabby start parody of her blog nyresolutio...</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>TN</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5009</th>\n",
              "      <td>female</td>\n",
              "      <td>Career</td>\n",
              "      <td>rt to have an excellent job before or after gr...</td>\n",
              "      <td>Paris  USA</td>\n",
              "      <td>TX</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>female</td>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>rt newyearsresolution on jan i am really going...</td>\n",
              "      <td>shenandoah conservatory</td>\n",
              "      <td>VA</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2493 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      gender   Resolution_Category  ... tweet_state tweet_region\n",
              "0     female      Health & Fitness  ...          CA         West\n",
              "1     female                 Humor  ...          NJ    Northeast\n",
              "4     female       Personal Growth  ...          PA    Northeast\n",
              "12    female      Health & Fitness  ...          NY    Northeast\n",
              "13    female       Personal Growth  ...          NY    Northeast\n",
              "...      ...                   ...  ...         ...          ...\n",
              "5006  female  Recreation & Leisure  ...          TN        South\n",
              "5007  female                 Humor  ...          NY    Northeast\n",
              "5008  female                 Humor  ...          TN        South\n",
              "5009  female                Career  ...          TX        South\n",
              "5010  female      Health & Fitness  ...          VA        South\n",
              "\n",
              "[2493 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEtMB4V7tuWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "3d271920-971c-45e7-d197-146f74897d4f"
      },
      "source": [
        "#Splitting into test and train.\n",
        "train, test = train_test_split(data,train_size=0.8, test_size=0.2, shuffle=True)\n",
        "#Using Sklearn, playing around with the CountVectorizer.\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", stop_words= \"english\",min_df= 50, binary= True)\n",
        "#Create vectorized train set\n",
        "X_train = vectorizer.fit_transform(train.text).toarray()\n",
        "X_train\n",
        "#Create vectorized test set\n",
        "X_test = vectorizer.transform(test.text).toarray()\n",
        "X_test"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXQ3b0ASedoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fef5f644-9c7a-4edb-b3ab-60c12f0573eb"
      },
      "source": [
        "NBmodel = MultinomialNB().fit(X_train, train.gender)\n",
        "predicted_1 = NBmodel.predict(X_test)\n",
        "np.mean(predicted_1 == test.gender)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5324027916251246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuFdSFVnemww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "16ee5f1a-5839-4980-f3fb-228a3c0fa935"
      },
      "source": [
        "\n",
        "# Import KFold model and define the amount of folds\n",
        "from sklearn.model_selection import KFold  # import model\n",
        "k = 5  # amount of folds. Also the default\n",
        "\n",
        "# Store an object of the KFold class in a variable with shuffle=True\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "\"\"\" \n",
        "Loop for the models and their designated folds (1 to k)\n",
        "and calculate the average of each of the 'k' models' accuracy\n",
        "using the NB classifier and k-fold model class.\n",
        "\"\"\"\n",
        "sum = 0 # sum to calculate average of model accuracies\n",
        "iteration = 1\n",
        "for train, test in kf.split(data): # Loop over K chunk of data splits\n",
        "    train_data = data.text[train] # New chunk of train data\n",
        "    test_data = data.text[test] # New chunk of test data\n",
        "    train_gender = data.gender[train]\n",
        "    test_gender = data.gender[test]\n",
        "\n",
        "\n",
        "    # Define the NB classifier to the train data\n",
        "    kd_classifier = MultinomialNB().fit(vectorizer.transform(train_data).toarray(), train_gender)\n",
        "\n",
        "    print(\"Iteration no.\", iteration, \"\\n\")\n",
        "    print(\"Accuracy of iteration number\", iteration, \":\", np.mean(NBmodel.predict(vectorizer.transform(train_gender).toarray()) == test_gender))\n",
        "    #kd_classifier.show_most_informative_features(10)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    sum += np.mean(NBmodel.predict(vectorizer.transform(test_data).toarray()) == test_gender) # Add current sum to total\n",
        "\n",
        "    iteration += 1 # Increment variable to display next iteration\n",
        "\n",
        "# Average calculated by the sum divided by the number of folds\n",
        "average = sum/k\n",
        "print(\"The accuracy average of the K-fold models:\", average)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration no. 1 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-baa6a3feda70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration no.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of iteration number\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNBmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m#kd_classifier.show_most_informative_features(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0;31m# as it will broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lengths must match to compare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Lengths must match to compare"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX-Bzu1deq5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCgEK2Kke1-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating labels for confusion matrix\n",
        "categories = list()\n",
        "for i in test.gender:\n",
        "  if i in categories:\n",
        "    pass\n",
        "  else:\n",
        "    categories.append(i)\n",
        "# A nice confusion matrix using seaborn.\n",
        "array = metrics.confusion_matrix(test.gender, predicted_1)\n",
        "plt.figure(figsize=(12,7))\n",
        "sn.set(font_scale=1) # for x/y label size\n",
        "sn.heatmap(array, annot=True, annot_kws={\"size\": 10}, xticklabels = categories, yticklabels = categories) # annot_kws is the size of the numbers.\n",
        "plt.show()\n",
        "print(array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9pbCcy2fCmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}