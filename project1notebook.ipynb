{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV3IQMyQnWOs",
        "colab_type": "code",
        "outputId": "69a45f4c-4a87-4ec1-e46b-7748ab03de2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "! git clone https://github.com/kaspergroenbek98/first-first-year-project.git\n",
        "### All import calls.\n",
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'first-first-year-project' already exists and is not an empty directory.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_r1ZxGuEu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3702fd02-8fb9-4e6e-e12f-5dd213f3a723"
      },
      "source": [
        "### Functions\n",
        "\n",
        "### START OF VOCABULARY ###\n",
        "def generate_vocabulary(data, featureData, vocabType): #featureData = data.gender\n",
        "    '''\n",
        "    Returns a list/vocabulary of len <= \"size\" based on the vocabType and the featureColumn specified\n",
        "    '''\n",
        "    size = 2000\n",
        "    # Only get large groups to get representative data\n",
        "    major_features = nltk.FreqDist(featureData).most_common(5)\n",
        "    major_masks = [featureData == f for (f, cnt) in major_features]\n",
        "    fqs = [nltk.FreqDist(word for line in data.text[mask] for word in line.split()) for mask in major_masks]\n",
        "    return list(vocabType(data, featureData, major_features, major_masks, fqs, size))\n",
        "\n",
        "def vocab_most_common(data, featureData, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises the most common words for each feature\n",
        "    '''\n",
        "    vocabulary = set()\n",
        "    # get the most common words in each freq dist. zip(*...) removes the counts from fd, and updates vocabulary ONLY with the words\n",
        "    for fd in fqs:\n",
        "        vocabulary.update(list(zip(*fd.most_common(size//len(major_features))))[0])\n",
        "    return vocabulary\n",
        "\n",
        "def vocab_unique(data, fCol, major_features, major_masks, fqs, size):\n",
        "    '''\n",
        "    Returns a vocabulary checklist for each tweet to check off (True/False).\n",
        "    Prioritises words which are uncommon in other features, but common in one feature\n",
        "    '''\n",
        "    major_mask = np.array(major_masks.sum(axis=0), dtype='bool')\n",
        "    fq = tweet_word_distribution(data[major_mask,6]) # Get a fq over the words used by all in the major categories\n",
        "    words = [word for (word, cnt) in fq.items() if cnt >= 20] # removes rarely mentioned words which probably arent indicative of a significant trend\n",
        "    priorityArray = []\n",
        "    for i, word in enumerate(words):\n",
        "        priorityArray.append([word])\n",
        "        #divide frequency of word in that state by the tweetcount from that state, and by how often that word is used in total by all states\n",
        "        score = max(fqs[fID][word]/(int(major_features[fID][1])*fq[word]) for fID in range(len(major_masks)))\n",
        "        priorityArray[i].append(score)\n",
        "    priorityArray.sort(key = lambda x: x[1], reverse=True) # Sort them based on their best score\n",
        "    vocabulary = list(zip(*priorityArray[:size]))[0] # Removes their scores\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END OF VOCABULARY ###\n",
        "    \n",
        "\n",
        "# def identify_hashtags(data):\n",
        "#     col = np.zeros((data.shape[0],1), 'str')\n",
        "#     data = np.append(data, col, axis=1)\n",
        "#     for i, text in enumerate(data[:,6]):\n",
        "#         results = re.findall(r\"#\\w+\", text) # Finds matches and returns them as an iterable\n",
        "#         if results:\n",
        "#             data[i,15] = ' '.join(results)\n",
        "#         else:\n",
        "#             data[i,15] = ''\n",
        "\n",
        "\n",
        "def divide_featureset(feature):\n",
        "    '''\n",
        "    Divides numpy featureset of (featureVector, classification) into a 80:10:10 train:dev:test set\n",
        "    '''\n",
        "    testSize = int(len(feature)*0.8)\n",
        "    train, rest = feature[:testSize], feature[testSize:]\n",
        "    restSize = len(rest)//2\n",
        "    dev, test = rest[:restSize], rest[restSize:]\n",
        "    return train, dev, test\n",
        "\n",
        "\n",
        "#RegEx cleaner for SKlearn (Maybe merge with Clean()).\n",
        "def cleaner(text):\n",
        "    stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "    text = text.lower() #All lowercase\n",
        "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
        "    text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub('b4', 'before', text)\n",
        "    text = text.strip(' ')\n",
        "    text = ''.join(char for char in text if char.isalpha() or char == ' ')\n",
        "    text = ''.join(stemmer.stem(text)) #Stem the words using SnowballStemmer\n",
        "    return text\n",
        "\n",
        "def delete_rows(dataset, column, ID):\n",
        "  \"\"\"\n",
        "  Takes a panda dataframe as first arg. The df column as second arg and the ID \n",
        "  as third. Returns a mask shoving only those values in the dataset.\n",
        "  \"\"\"\n",
        "  mask = dataset[dataset[str(column)] == str(ID)]\n",
        "  return mask\n",
        "\n",
        "\"\"\"\n",
        "Needs revisiting (below)\n",
        "\"\"\"\n",
        "# def clean(tweet):\n",
        "#     stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "#     # Remove all stopwords, non-alphabet words (except spaces), and stem the words\n",
        "#     for text in tweet:\n",
        "#     text = text.lower()\n",
        "#     text = ''.join(char for char in tweet if char.isalpha() or char == ' ')\n",
        "#     text = ' '.join(stemmer.stem(text))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Needs a concrete decision on future use of the code below\n",
        "\"\"\"\n",
        "# def generate_features(data, vocabulary, fCol):\n",
        "#     \"\"\" \n",
        "#     Creates tuples with a vector containing boolean values depending on whether\n",
        "#     or not the word is in the tweet - along with the label of the tweet.\n",
        "#     \"\"\"\n",
        "#     features = [(tweet_features(d.split(), vocabulary), c) for (d,c) in zip(data[:,6], data[:,fCol])] # column 6 is text data, column 2 is gender data\n",
        "#     return features"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNeeds a concrete decision on future use of the code below\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgs7KbInxrx",
        "colab_type": "code",
        "outputId": "94f520cc-3515-48e1-a8ab-5c72e8512d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Using pandas to load the data file into a CSV.\n",
        "# ISO-8859-1 is the encoding for tweets.\n",
        "df = pd.read_csv(\"first-first-year-project/data.csv\", encoding = \"ISO-8859-1\")\n",
        "#First step to cleaning the data, removing the categories we dont need.\n",
        "data = df.drop([\"other_topic\",\"resolution_topics\", \"tweet_coord\", \"tweet_created\",\"tweet_id\", \"name\", \"retweet_count\", \"tweet_date\", \"user_timezone\"], axis = 1)\n",
        "#Cleaning all the text in our data with our newly build cleaner() function.\n",
        "for i in range(len(data.text)):\n",
        "  data.text[i] = cleaner(data.text[i])\n",
        "\n",
        "data.text[1]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'newyearsresolution finally master part of kitchen sink'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEtMB4V7tuWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c907ba6d-a2bb-4191-bca1-a822d95d904d"
      },
      "source": [
        "# Vectorize using sklearn based on a vocabulary\n",
        "featureData = data.gender\n",
        "vectorizer = CountVectorizer(analyzer=\"word\", stop_words= \"english\", min_df = 20, binary= True, vocabulary = generate_vocabulary(data, featureData, vocab_most_common))\n",
        "\n",
        "#Here we use term frequency to downscale the importance of words occuring many times in a tweet. That way we hopefully get more weighted words. \n",
        "tfidf_transformer = TfidfTransformer()\n",
        "features = vectorizer.fit_transform(data.text)\n",
        "tfidf_features = tfidf_transformer.fit_transform(features)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "# Store an object of the KFold class in a variable with shuffle=True\n",
        "k= 10\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "showReport = True\n",
        "\n",
        "# \"\"\" \n",
        "# Loop for the models and their designated folds (1 to k)\n",
        "# and calculate the average of each of the 'k' models' accuracy\n",
        "# using the NB classifier and k-fold model class.\n",
        "# \"\"\"\n",
        "count = 0 # sum to calculate average of model accuracies\n",
        "for i, (train, test) in enumerate(kf.split(features)): # Loop over K chunk of data splits\n",
        "    #Accuracy using term frequency. (Odd that its lower, but we do have a small data set)\n",
        "    NBmodel = MultinomialNB().fit(tfidf_features[train], featureData[train])\n",
        "    predicted = NBmodel.predict(tfidf_features[test])\n",
        "    accuracy = np.mean(predicted == featureData[test])\n",
        "    count += accuracy\n",
        "\n",
        "    #A classification_report\n",
        "    print(\"Iteration no.\", i, \"\\n\")\n",
        "    print(\"Accuracy of iteration number\", i, \":\", accuracy)\n",
        "    if showReport: print(metrics.classification_report(data.gender[test], predicted))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "average = count/k\n",
        "print(\"The accuracy average of the K-fold models:\", average)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration no. 0 \n",
            "\n",
            "Accuracy of iteration number 0 : 0.5876494023904383\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.60      0.57      0.59       254\n",
            "        male       0.58      0.60      0.59       248\n",
            "\n",
            "    accuracy                           0.59       502\n",
            "   macro avg       0.59      0.59      0.59       502\n",
            "weighted avg       0.59      0.59      0.59       502\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 1 \n",
            "\n",
            "Accuracy of iteration number 1 : 0.6027944111776448\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.63      0.58      0.60       260\n",
            "        male       0.58      0.63      0.60       241\n",
            "\n",
            "    accuracy                           0.60       501\n",
            "   macro avg       0.60      0.60      0.60       501\n",
            "weighted avg       0.60      0.60      0.60       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 2 \n",
            "\n",
            "Accuracy of iteration number 2 : 0.562874251497006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.54      0.54      0.54       240\n",
            "        male       0.58      0.58      0.58       261\n",
            "\n",
            "    accuracy                           0.56       501\n",
            "   macro avg       0.56      0.56      0.56       501\n",
            "weighted avg       0.56      0.56      0.56       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 3 \n",
            "\n",
            "Accuracy of iteration number 3 : 0.5788423153692615\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.53      0.58      0.56       226\n",
            "        male       0.63      0.57      0.60       275\n",
            "\n",
            "    accuracy                           0.58       501\n",
            "   macro avg       0.58      0.58      0.58       501\n",
            "weighted avg       0.58      0.58      0.58       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 4 \n",
            "\n",
            "Accuracy of iteration number 4 : 0.5988023952095808\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.60      0.59      0.59       248\n",
            "        male       0.60      0.60      0.60       253\n",
            "\n",
            "    accuracy                           0.60       501\n",
            "   macro avg       0.60      0.60      0.60       501\n",
            "weighted avg       0.60      0.60      0.60       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 5 \n",
            "\n",
            "Accuracy of iteration number 5 : 0.5988023952095808\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.59      0.62      0.61       247\n",
            "        male       0.61      0.57      0.59       254\n",
            "\n",
            "    accuracy                           0.60       501\n",
            "   macro avg       0.60      0.60      0.60       501\n",
            "weighted avg       0.60      0.60      0.60       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 6 \n",
            "\n",
            "Accuracy of iteration number 6 : 0.6047904191616766\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.62      0.60      0.61       259\n",
            "        male       0.59      0.61      0.60       242\n",
            "\n",
            "    accuracy                           0.60       501\n",
            "   macro avg       0.60      0.61      0.60       501\n",
            "weighted avg       0.61      0.60      0.60       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 7 \n",
            "\n",
            "Accuracy of iteration number 7 : 0.6387225548902196\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.66      0.61      0.64       259\n",
            "        male       0.62      0.67      0.64       242\n",
            "\n",
            "    accuracy                           0.64       501\n",
            "   macro avg       0.64      0.64      0.64       501\n",
            "weighted avg       0.64      0.64      0.64       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 8 \n",
            "\n",
            "Accuracy of iteration number 8 : 0.5728542914171657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.56      0.53      0.54       240\n",
            "        male       0.59      0.61      0.60       261\n",
            "\n",
            "    accuracy                           0.57       501\n",
            "   macro avg       0.57      0.57      0.57       501\n",
            "weighted avg       0.57      0.57      0.57       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration no. 9 \n",
            "\n",
            "Accuracy of iteration number 9 : 0.5828343313373253\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      female       0.60      0.60      0.60       260\n",
            "        male       0.57      0.56      0.57       241\n",
            "\n",
            "    accuracy                           0.58       501\n",
            "   macro avg       0.58      0.58      0.58       501\n",
            "weighted avg       0.58      0.58      0.58       501\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The accuracy average of the K-fold models: 0.5928966767659899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoNtCVpU6Hur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}